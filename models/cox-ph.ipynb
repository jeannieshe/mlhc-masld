{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cox-PH and DeepSurv\n",
    "\n",
    "In this notebook we will train the [Cox-PH method](http://jmlr.org/papers/volume20/18-424/18-424.pdf), also known as [DeepSurv](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1).\n",
    "We will use the METABRIC data sets as an example\n",
    "\n",
    "A more detailed introduction to the `pycox` package can be found in [this notebook](https://nbviewer.jupyter.org/github/havakv/pycox/blob/master/examples/01_introduction.ipynb) about the `LogisticHazard` method.\n",
    "\n",
    "The main benefit Cox-CC (and the other Cox methods) has over Logistic-Hazard is that it is a continuous-time method, meaning we do not need to discretize the time scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment to install `sklearn-pandas`\n",
    "! pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We load the METABRIC data set and split in train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    print(df_train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = metabric.read_df()\n",
    "df_test = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "df_val = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASH dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../clean_data/nafl/combined.large.nafl.csv\")\n",
    "\n",
    "X = data.drop(columns=['DaysUntilFirstProgression', 'Outcome'])\n",
    "Y = data[['StudyID', 'DaysUntilFirstProgression', 'Outcome']]\n",
    "\n",
    "X = X.set_index('StudyID')\n",
    "Y = Y.set_index('StudyID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Outcome' in X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = ['Lab', 'Med', 'Code']\n",
    "print([x for x in X.columns if not any(f in x for f in foo)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['Outcome'] = Y['Outcome'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_torch, Y_torch, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transforms\n",
    "We have 9 covariates, in addition to the durations and event indicators.\n",
    "\n",
    "We will standardize the 5 numerical covariates, and leave the binary variables as is. As variables needs to be of type `'float32'`, as this is required by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "# x_val = x_mapper.transform(df_val).astype('float32')\n",
    "# x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "x_train = X_train.values.astype('float32')\n",
    "x_val = X_val.values.astype('float32')\n",
    "x_test = X_test.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting the dataframe mapper method to standardize\n",
    "cols_numerical = [x for x in X.columns if any(keyword in x for keyword in ['BMI', 'Age', 'Lab'])]\n",
    "cols_other = [x for x in X.columns if x not in cols_numerical]\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_numerical]\n",
    "leave = [(col, None) for col in cols_other]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_mapper.fit_transform(X_train).astype('float32')\n",
    "x_val = x_mapper.transform(X_val).astype('float32')\n",
    "x_test = x_mapper.transform(X_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_feat = [feat for feat in X.columns if 'Lab' in feat]\n",
    "numerical_feat = ['mean_BMI', 'last_BMI', 'FirstNAFL.Age.90']\n",
    "numerical_feat.extend(lab_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numerical_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to only standardize the numerical columns and reattach to the rest of the dataframe\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def standardize_numerical(dataframe, num_feat=numerical_feat, training_set=True):\n",
    "    \"\"\"\n",
    "    dataframe: Pandas DataFrame\n",
    "\n",
    "    Returns: a processed DataFrame where the numerical features have been standardized and the categorical features remain the same.\n",
    "    \"\"\"\n",
    "    if training_set:\n",
    "        scaled = scaler.fit_transform(dataframe[num_feat])\n",
    "    else:\n",
    "        scaled = scaler.transform(dataframe[num_feat])\n",
    "        \n",
    "    scaled_df = pd.DataFrame(scaled, columns=num_feat, index=dataframe.index)\n",
    "    cat = dataframe.drop(columns=num_feat)\n",
    "    processed = pd.concat([scaled_df, cat], axis=1)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize our features\n",
    "X_train_scaled = standardize_numerical(X_train, training_set=True)\n",
    "X_val_scaled = standardize_numerical(X_val, training_set=False)\n",
    "X_test_scaled = standardize_numerical(X_test, training_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.DataFrame(x_train, columns=cols_numerical + cols_other, index = X_train_scaled.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need no label transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target = lambda df: (df['DaysUntilFirstProgression'].values, df['Outcome'].values)\n",
    "y_train = get_target(y_train)\n",
    "y_val = get_target(y_val)\n",
    "durations_test, events_test = get_target(y_test)\n",
    "val = x_val, y_val\n",
    "\n",
    "# get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "\n",
    "# y_train = get_target(df_train)\n",
    "# y_val = get_target(df_val)\n",
    "# durations_test, events_test = get_target(df_test)\n",
    "# val = x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = (y_train[0].astype('float32'), y_train[1].astype('int32'))\n",
    "y_val_new = (y_val[0].astype('float32'), y_val[1].astype('int32'))\n",
    "durations_test = durations_test.astype('float32')\n",
    "events_test = events_test.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net\n",
    "\n",
    "We create a simple MLP with two hidden layers, ReLU activations, batch norm and dropout. \n",
    "Here, we just use the `torchtuples.practical.MLPVanilla` net to do this.\n",
    "\n",
    "Note that we set `out_features` to 1, and that we have not `output_bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 32]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.1\n",
    "output_bias = False\n",
    "\n",
    "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                              dropout, output_bias=output_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train the model we need to define an optimizer. You can choose any `torch.optim` optimizer, but here we instead use one from `tt.optim` as it has some added functionality.\n",
    "We use the `Adam` optimizer, but instead of choosing a learning rate, we will use the scheme proposed by [Smith 2017](https://arxiv.org/pdf/1506.01186.pdf) to find a suitable learning rate with `model.lr_finder`. See [this post](https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoxPH(net, tt.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "lrfinder = model.lr_finder(x_train, y_train_new, batch_size, tolerance=10)\n",
    "_ = lrfinder.plot()\n",
    "# plt.savefig(\"batch_loss_learning_rate.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfinder.get_best_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, this learning rate is a little high, so we instead set it manually to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.set_lr(0.003) # 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the `EarlyStopping` callback to stop training when the validation loss stops improving. After training, this callback will also load the best performing model in terms of validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 512\n",
    "callbacks = [tt.callbacks.EarlyStopping()]\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "log = model.fit(x_train, y_train_new, batch_size, epochs, callbacks, verbose,\n",
    "                val_data=val, val_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = log.plot()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Training Epochs')\n",
    "# plt.savefig(\"loss_per_epoch.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the partial log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.partial_log_likelihood(*val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = x_train, y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.partial_log_likelihood(*train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "For evaluation we first need to obtain survival estimates for the test set.\n",
    "This can be done with `model.predict_surv` which returns an array of survival estimates, or with `model.predict_surv_df` which returns the survival estimates as a dataframe.\n",
    "\n",
    "However, as `CoxPH` is semi-parametric, we first need to get the non-parametric baseline hazard estimates with `compute_baseline_hazards`. \n",
    "\n",
    "Note that for large datasets the `sample` argument can be used to estimate the baseline hazard on a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.compute_baseline_hazards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = model.predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_studyid = y_test.reset_index(inplace=False)\n",
    "pos_outcome = y_test_studyid.index[y_test['Outcome'] == 1].tolist()\n",
    "neg_outcome = y_test_studyid.index[y_test['Outcome'] == 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate one curve for all patients with positive progression\n",
    "pos_surv = surv[pos_outcome]\n",
    "pos_surv_avg = pos_surv.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate one curve for all patients with negative progression\n",
    "neg_surv = surv[neg_outcome]\n",
    "neg_surv_avg = neg_surv.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pos_surv_avg, label='Progressed Patients')\n",
    "plt.plot(neg_surv_avg, label='Censored Patients')\n",
    "\n",
    "plt.ylabel('Average S(t | x)')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.title('Average Survival Prediction')\n",
    "plt.legend()\n",
    "# plt.savefig(\"average_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.iloc[:, pos_outcome[0:10]].plot(legend=None)\n",
    "plt.ylabel('S(t | x)')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.title('Survival Prediction for 10 Progressed Patients')\n",
    "# plt.legend(False)\n",
    "# surv.iloc[:, :].plot()\n",
    "# plt.ylabel('S(t | x)')\n",
    "# _ = plt.xlabel('Time')\n",
    "# plt.savefig(\"progressed_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.iloc[:, neg_outcome[0:10]].plot(legend=None)\n",
    "plt.ylabel('S(t | x)')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.title('Survival Prediction for 10 Censored Patients')\n",
    "# plt.savefig(\"censored_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.iloc[:, 1000:1010].plot()\n",
    "plt.ylabel('S(t | x)')\n",
    "_ = plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can use the `EvalSurv` class for evaluation the concordance, brier score and binomial log-likelihood. Setting `censor_surv='km'` means that we estimate the censoring distribution by Kaplan-Meier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.concordance_td()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_train = model.predict_surv_df(x_train)\n",
    "\n",
    "# Step 2: Create EvalSurv object using training labels\n",
    "ev_train = EvalSurv(\n",
    "    surv_train,\n",
    "    durations=y_train_new[0],\n",
    "    events=y_train_new[1],\n",
    "    censor_surv='km'\n",
    ")\n",
    "\n",
    "# # Step 3: Compute Concordance Index\n",
    "# c_index_train = ev_train.concordance_td('antolini')  # or 'adj_antolini', 'uno', etc.\n",
    "# print(f\"C-index (train): {c_index_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_train.concordance_td()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "_ = ev.brier_score(time_grid).plot()\n",
    "\n",
    "# plt.figure(figsize=(6.4, 4.8))\n",
    "plt.xlabel('Time t (days)')\n",
    "plt.ylabel('Brier Score')\n",
    "plt.ylim(0, 0.9)\n",
    "plt.title('Cox PH DeepSurv Brier Score Over Time')\n",
    "# plt.savefig(\"results/deepsurv_brier_score.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_brier_score(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_train.integrated_brier_score(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_nbll(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_train.integrated_nbll(time_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt to produce SHAP scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a scalar output for the SHAP\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "net.to('cpu')\n",
    "\n",
    "# # Background dataset: small sample from training data\n",
    "# X_background = torch.tensor(x_train[:100], dtype=torch.float32)\n",
    "\n",
    "# # Test samples for SHAP analysis\n",
    "# X_explain = torch.tensor(x_test[:50], dtype=torch.float32)\n",
    "\n",
    "# DeepExplainer works directly with the PyTorch model\n",
    "explainer = shap.DeepExplainer(net, torch.tensor(x_train, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values â€” returns shape (n_samples, n_features)\n",
    "shap_values = explainer.shap_values(torch.tensor(x_test, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed = shap_values.squeeze(-1)\n",
    "shap_values_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'results/coxph_shap_values_scaledx.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    # Use pickle.dump to serialize and write the data\n",
    "    pickle.dump(shap_values_squeezed, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pickle.load('results/coxph_shap_values_scaledx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'results/coxph_shap_values_scaledx.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    foo = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "shap.summary_plot(shap_values_squeezed, x_test, feature_names=X.columns, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with human readable names\n",
    "features = ['Transaminase-SGOT AST Blood Test', 'Iron Binding Capacity Test', 'Chest pain', 'Essential (primary) hypertension', 'Hemoglobin Blood Test', 'Very low-density lipoprotein Blood Test', 'Type 2 diabetes mellitus', 'Mean Corpuscular Hemoglobin Blood Test', 'Abnormal results of liver function studies', 'Vitamin D Blood Test','Hematocrit Blood Test','Mean corpuscular volume Blood Test','Other specified disorders involving the immune mechanism','Most recent BMI before MASLD diagnosis','Platelet Blood Test','Cardiac Risk Ratio','Red blood cell count Blood Test','Vitamin D deficiency, unspecified','Other nonspecific abnormal finding of lung field','Other specified counseling' ]\n",
    "shap_values_top = shap_values_squeezed[:, :20]\n",
    "x_test_top = x_test[:, :20]\n",
    "# features_top = features[:20]\n",
    "features_top = X.columns[:20]\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "shap.summary_plot(shap_values_top, x_test_top, feature_names=features_top, show=False, max_display=10)\n",
    "plt.xlabel('SHAP value (impact on model output)')\n",
    "plt.title('Cox PH DeepSurv Influential Features')\n",
    "# plt.savefig('results/coxph_shap.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with human readable names\n",
    "features = ['Transaminase-SGOT AST Blood Test', 'Iron Binding Capacity Test', 'Chest pain', 'Essential (primary) hypertension', 'Hemoglobin Blood Test', 'Very low-density lipoprotein Blood Test', 'Type 2 diabetes mellitus', 'Mean Corpuscular Hemoglobin Blood Test', 'Abnormal results of liver function studies', 'Vitamin D Blood Test','Hematocrit Blood Test','Mean corpuscular volume Blood Test','Other specified disorders involving the immune mechanism','Most recent BMI before MASLD diagnosis','Platelet Blood Test','Cardiac Risk Ratio','Red blood cell count Blood Test','Vitamin D deficiency, unspecified','Other nonspecific abnormal finding of lung field','Other specified counseling' ]\n",
    "shap_values_top = shap_values_squeezed[:, :20]\n",
    "x_test_top = x_test[:, :20]\n",
    "features_top = features[:20]\n",
    "# features_top = X.columns[:20]\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "shap.summary_plot(shap_values_top, x_test_top, feature_names=features_top, show=False, max_display=10)\n",
    "plt.xlabel('SHAP value (impact on model output)')\n",
    "plt.gca().tick_params(axis='y', labelsize=11)\n",
    "plt.title('Cox PH DeepSurv Influential Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/coxph_shap.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean absolute SHAP values per feature\n",
    "shap_mean_abs = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "# Create a Series for easy sorting\n",
    "shap_series = pd.Series(shap_mean_abs, index=X.columns)\n",
    "\n",
    "# Get top 20 feature names\n",
    "top20_features = shap_series.sort_values(ascending=False).head(20).index.tolist()\n",
    "top20_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top features according to shap score\n",
    "# rank features by mean absolute SHAP value\n",
    "# Calculate mean absolute SHAP value for each feature\n",
    "feature_names = X.columns\n",
    "\n",
    "mean_abs_shap = np.abs(shap_values_squeezed).mean(axis=0)\n",
    "\n",
    "# Get indices of top features\n",
    "top_indices = np.argsort(mean_abs_shap)[::-1]  # descending order\n",
    "\n",
    "# Get corresponding feature names and importance values\n",
    "top_features = [(feature_names[i], mean_abs_shap[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10_features = [x[0] for x in top_features[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features = [x[0] for x in top_features[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the mean SHAP value for each feature (not absolute)\n",
    "mean_shap = shap_values_squeezed.mean(axis=0)\n",
    "\n",
    "# 2. Get indices of top 10 positive and top 10 negative impact features\n",
    "top_positive_indices = np.argsort(mean_shap)[-10:]  # most positive\n",
    "top_negative_indices = np.argsort(mean_shap)[:10]   # most negative\n",
    "\n",
    "# 3. Retrieve feature names and their SHAP values\n",
    "top_positive_features = [(feature_names[i], mean_shap[i]) for i in reversed(top_positive_indices)]\n",
    "top_negative_features = [(feature_names[i], mean_shap[i]) for i in top_negative_indices]\n",
    "\n",
    "pos_names = [x[0] for x in top_positive_features]\n",
    "# pos_names\n",
    "\n",
    "neg_names = [x[0] for x in top_negative_features]\n",
    "# neg_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot survival curves of populations split on the most descriptive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting on median for continuous variables\n",
    "# X['Lab_2091-7'].mean()\n",
    "np.sum([1 for x in X['Lab_2091-7'].values if x > 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([1 for x in X['Code_R07.9'].values if x != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_studyid.set_index('StudyID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = X[X['Lab_2091-7'] > 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_studyid = y_test.reset_index(inplace=False)\n",
    "pos_outcome = y_test_studyid.index[y_test['Lab_2091-7'] == 1].tolist()\n",
    "neg_outcome = y_test_studyid.index[y_test['Outcome'] == 0].tolist()\n",
    "\n",
    "pos_surv = surv[pos_outcome]\n",
    "pos_surv_avg = pos_surv.mean(axis=1)\n",
    "\n",
    "plt.plot(pos_surv_avg, label='Progressed Patients')\n",
    "plt.plot(neg_surv_avg, label='Censored Patients')\n",
    "\n",
    "plt.ylabel('Average S(t | x)')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.title('Average Survival Prediction')\n",
    "plt.legend()\n",
    "# plt.savefig(\"average_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i can only plot the survival estimates for the x_test\n",
    "y_test_id = y_test.reset_index(inplace=False)\n",
    "pos_outcome = y_test_studyid.index[X_test_scaled['Lab_2091-7'] > 0].tolist()\n",
    "neg_outcome = y_test_studyid.index[X_test_scaled['Lab_2091-7'] <= 0].tolist()\n",
    "\n",
    "pos_surv = surv[pos_outcome]\n",
    "pos_surv_avg = pos_surv.mean(axis=1)\n",
    "\n",
    "plt.plot(pos_surv_avg, label='Patients with Positive Lab_2091-7')\n",
    "plt.plot(neg_surv_avg, label='Censored Patients')\n",
    "\n",
    "plt.ylabel('Average S(t | x)')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.title('Average Survival Prediction')\n",
    "plt.legend()\n",
    "# plt.savefig(\"average_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled['Lab_2091-7'].median() # mean: -0.05, median: -0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diabetes, insulin, and VLDL\n",
    "# np.sum([1 for x in X_test_scaled['MedType_Code_EPIC-MED_119019'] if not x]) # insulin\n",
    "# np.sum([1 for x in X_test_scaled['Code_E11.9'] if not x]) # diabetes\n",
    "np.sum([1 for x in X_test['Lab_2091-7'] if x > 0]) # vldl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(X_test['MedType_Code_EPIC-MED_119019'])\n",
    "# target_ids = X_test.index[X_test['Code_E11.9'] == True].tolist()\n",
    "target_ids = X_test.index[X_test['Code_R94.5'] == True].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studyid_to_col = {study_id: i for i, study_id in enumerate(X_test.index)}\n",
    "target_cols = [studyid_to_col[sid] for sid in target_ids if sid in studyid_to_col]\n",
    "pos_surv = surv.iloc[:, target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_surv = surv.drop(surv.columns[target_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_surv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_surv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_studyid = y_test.reset_index(inplace=False)\n",
    "# pos_outcome = y_test_studyid.index[X_test['Lab_2091-7'] == 1].tolist()\n",
    "# neg_outcome = y_test_studyid.index[y_test['Outcome'] == 0].tolist()\n",
    "\n",
    "# pos_surv = surv[pos_outcome]\n",
    "pos_surv_avg = pos_surv.mean(axis=1)\n",
    "neg_surv_avg = neg_surv.mean(axis=1)\n",
    "\n",
    "plt.plot(pos_surv_avg, label='Patients Diagnosed with Abnormal Liver Function (n=609)')\n",
    "plt.plot(neg_surv_avg, label='Patients Diagnosed without Abnormal Liver Function (n=2866)')\n",
    "\n",
    "plt.ylabel('Average S(t | x)')\n",
    "_ = plt.xlabel('Time t (days)')\n",
    "plt.title('Predicted Time to Progression Stratified by Abnormal Liver Diagnosis')\n",
    "plt.legend()\n",
    "plt.savefig(\"results/coxph_abnormalliver.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check common shap features\n",
    "nn = {'Lab_4679-7',\n",
    " 'Lab_14338-8',\n",
    " 'Lab_2132-9',\n",
    " 'Lab_6768-6',\n",
    " 'Code_Z23',\n",
    " 'Lab_6690-2',\n",
    " 'Lab_2093-3',\n",
    " 'MedType_Code_EPIC-MED_10328',\n",
    " 'Lab_13457-7',\n",
    " 'Lab_2571-8',\n",
    " 'MedType_Code_EPIC-MED_27698',\n",
    " 'Lab_19153-6',\n",
    " 'Lab_2501-5',\n",
    " 'Lab_786-4',\n",
    " 'Code_E78.5',\n",
    " 'Lab_XC5-9',\n",
    " 'Lab_2502-3',\n",
    " 'MedType_Code_EPIC-MED_40900',\n",
    " 'Lab_2089-1',\n",
    " 'Lab_789-8'}\n",
    "\n",
    "cox = {'Lab_1920-8',\n",
    " 'Lab_2500-7',\n",
    " 'Code_R07.9',\n",
    " 'Code_I10',\n",
    " 'Lab_718-7',\n",
    " 'Lab_2091-7',\n",
    " 'Code_E11.9',\n",
    " 'Lab_785-6',\n",
    " 'Code_R94.5',\n",
    " 'Lab_62292-8',\n",
    " 'Lab_4544-3',\n",
    " 'Lab_787-2',\n",
    " 'Code_D89.89',\n",
    " 'last_BMI',\n",
    " 'Lab_777-3',\n",
    " 'Lab_9830-1',\n",
    " 'Lab_789-8',\n",
    " 'Code_E55.9',\n",
    " 'Code_R91.8',\n",
    " 'Code_Z71.89'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = nn.intersection(cox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pycox-env]",
   "language": "python",
   "name": "conda-env-pycox-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
